{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faPsiNvx5nBx"
      },
      "outputs": [],
      "source": [
        "## Install Packages\n",
        "\n",
        "!pip install matplotlib\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "!pip install ipympl\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "%matplotlib ipympl\n",
        "\n",
        "# Install mmdetection\n",
        "!pip install -U openmim\n",
        "!mim install mmengine\n",
        "!mim install \"mmcv>=2.0.0\"\n",
        "!mim install mmdet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4Mhcdb56KfO"
      },
      "outputs": [],
      "source": [
        "# Download a checkpoint, config and a sample video.\n",
        "!wget http://codingai-s3.s3.amazonaws.com/public/crowdhuman_demo_config.py\n",
        "!wget http://codingai-s3.s3.amazonaws.com/public/small_shinjuku_1.mp4\n",
        "!wget http://codingai-s3.s3.amazonaws.com/public/crowdhuman_epoch_24.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSPEzHW8IVUO",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Hidden Functions\n",
        "\n",
        "import imageio, cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "\"\"\"\n",
        "In google colab, we can only display videos in certain formats. the function below\n",
        "rewrites a video into that format and saves it at tmp.mp4\n",
        "\"\"\"\n",
        "class VideoWriter:\n",
        "    def __init__(self,name,fps=10):\n",
        "        self.name=f'{name}.mp4'\n",
        "        self.video = imageio.get_writer(self.name, mode='I', fps=fps, codec='libx264', bitrate='1M')\n",
        "    def write(self,img):\n",
        "        \"\"\"\n",
        "        BGR input\n",
        "        \"\"\"\n",
        "        self.video.append_data(img[:,:,::-1])\n",
        "    def release(self):\n",
        "        print(f\"Wrote to {self.name}\")\n",
        "        self.video.close()\n",
        "def video_rewrite(vid):\n",
        "    \"\"\"\n",
        "    rewrites a video in format for google colab \n",
        "    \"\"\"\n",
        "    cam = cv2.VideoCapture(vid)\n",
        "    vw = VideoWriter('tmp',fps=10)\n",
        "    while True:\n",
        "        ret,img = cam.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        vw.write(img)\n",
        "    vw.release()\n",
        "\n",
        "\n",
        "def get_still_images_from_video(vid_name,inds = {0}):\n",
        "    \"\"\"\n",
        "    This function is to retrieve a few still images from a video\n",
        "    for demonstration purposes\n",
        "    \"\"\"\n",
        "    cam = cv2.VideoCapture(vid_name)\n",
        "    imgs = []\n",
        "    i = 0\n",
        "    while True:\n",
        "      ret,img = cam.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      if i in inds:\n",
        "        imgs.append(img)\n",
        "      i+=1\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def img_grid(images):\n",
        "    return np.concatenate(images,axis=0)\n",
        "### ALL OTHER FUNCTIONS\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "def visualize_images(model, images):\n",
        "  vis_images = []\n",
        "  for image in images:\n",
        "    result = inference_detector(model, image, test_pipeline=test_pipeline)\n",
        "\n",
        "    visualizer.add_datasample(\n",
        "            'result',\n",
        "            image,\n",
        "            data_sample=result,\n",
        "            draw_gt=False)\n",
        "    vis_image = visualizer.get_image()\n",
        "    vis_images.append(vis_image)\n",
        "  return vis_images\n",
        "\n",
        "\n",
        "DEFINITION = {\n",
        "    0:'person'\n",
        "}\n",
        "class Detection:\n",
        "  def __init__(self,box,label,score):\n",
        "    self.box=box\n",
        "    self.label=DEFINITION[label]\n",
        "    self.score=score\n",
        "  def __str__(self):\n",
        "    return(f\"{self.label} | {self.score} | {self.box}\")\n",
        "\n",
        "def get_detections_from_result(result):\n",
        "    instances = result.pred_instances.numpy().cpu()\n",
        "    return [Detection(box,label,score) for box,label,score in zip(instances['bboxes'],instances['labels'],instances['scores'])]\n",
        "def visualize_boxes(image,dets,color = [0,255,0]):\n",
        "    \"\"\"\n",
        "    Visualizes detections\n",
        "    \"\"\"\n",
        "    new_image = image.copy()\n",
        "    for det in dets:\n",
        "        box = [int(round(j)) for j in det.box]\n",
        "        cv2.rectangle(new_image,box[:2],box[2:],color,3)\n",
        "    return new_image\n",
        "def iou(x1_1, y1_1, x2_1, y2_1,\n",
        "        x1_2, y1_2, x2_2, y2_2):\n",
        "    intersection_width = max(min(x2_1, x2_2) - max(x1_1, x1_2),0)\n",
        "    intersection_height = max(min(y2_1, y2_2) - max(y1_1, y1_2),0)\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "    return intersection_area / union_area\n",
        "COLORS = [\n",
        "    (255, 0, 0),     # Red\n",
        "    (255, 127, 0),   # Orange\n",
        "    (255, 255, 0),   # Yellow\n",
        "    (191, 255, 0),   # Chartreuse green\n",
        "    (0, 255, 0),     # Green\n",
        "    (0, 255, 127),   # Spring green\n",
        "    (0, 255, 255),   # Cyan / Aqua\n",
        "    (0, 191, 255),   # Sky blue\n",
        "    (0, 0, 255),     # Blue\n",
        "    (127, 0, 255),   # Purple / Violet\n",
        "    (255, 0, 255),   # Magenta / Fuchsia\n",
        "    (255, 0, 191),   # Hot pink\n",
        "    (255, 0, 127),   # Pink\n",
        "    (255, 63, 0),    # Dark orange\n",
        "    (255, 127, 63),  # Light orange\n",
        "    (255, 191, 127), # Peach\n",
        "    (255, 255, 127), # Pastel yellow\n",
        "    (191, 255, 127), # Light green\n",
        "    (127, 255, 127), # Mint green\n",
        "    (127, 255, 191), # Seafoam green\n",
        "    (127, 255, 255), # Light blue\n",
        "    (127, 191, 255), # Periwinkle\n",
        "    (127, 127, 255), # Lavender\n",
        "    (191, 127, 255), # Lilac\n",
        "    (255, 127, 255), # Pale pink\n",
        "    (255, 127, 191), # Salmon pink\n",
        "    (255, 127, 127), # Coral\n",
        "    (255, 191, 191), # Pastel pink\n",
        "    (255, 191, 127), # Apricot\n",
        "    (255, 191, 191)  # Blush pink\n",
        "]\n",
        "def iou(x1_1, y1_1, x2_1, y2_1,\n",
        "        x1_2, y1_2, x2_2, y2_2):\n",
        "    intersection_width = max(min(x2_1, x2_2) - max(x1_1, x1_2),0)\n",
        "    intersection_height = max(min(y2_1, y2_2) - max(y1_1, y1_2),0)\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "    return intersection_area / union_area\n",
        "class TrackedObject:\n",
        "    def __init__(self,idx,box,reset_threshold=60):\n",
        "        self.idx = idx\n",
        "        self.box = box\n",
        "        self.unused_count = 0\n",
        "        self.start_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "        self.current_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]\n",
        "        self.has_crossed = False\n",
        "        self.direction = ''\n",
        "        self.reset_threshold = reset_threshold\n",
        "        self.cross_count = 0\n",
        "\n",
        "    def update(self,box):\n",
        "        self.box = box\n",
        "        self.unused_count = 0\n",
        "        self.current_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "        if self.has_crossed:\n",
        "            self.cross_count+=1\n",
        "            if self.cross_count>=self.reset_threshold:\n",
        "                self.has_crossed=False\n",
        "                self.start_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "    def unused(self):\n",
        "        self.unused_count+=1\n",
        "    def get_has_crossed(self):\n",
        "        return self.has_crossed\n",
        "    def mark_crossed(self):\n",
        "        self.has_crossed = True\n",
        "        self.cross_count = 0\n",
        "        self.up_or_down()\n",
        "    def up_or_down(self):\n",
        "        y0 = self.start_centroid[1]\n",
        "        y1 = self.current_centroid[1]\n",
        "        self.direction='up'\n",
        "        if y1>y0:\n",
        "            self.direction = 'down'\n",
        "    def get_unused_count(self):\n",
        "        return self.unused_count\n",
        "    def get_direction(self):\n",
        "        return self.direction\n",
        "    def get_box(self):\n",
        "        return self.box\n",
        "    def get_idx(self):\n",
        "        return self.idx\n",
        "    def get_has_crossed(self):\n",
        "        return self.has_crossed\n",
        "    def get_start_centroid(self):\n",
        "        return self.start_centroid\n",
        "    def get_current_centroid(self):\n",
        "        return self.current_centroid\n",
        "    def get_cross_count(self):\n",
        "        return self.cross_count\n",
        "class Tracker:\n",
        "    def __init__(self,unused_threshold=10,printout=False,object_reset_threshold=100):\n",
        "        self.objects = []\n",
        "        self.count = 0\n",
        "        self.unused_threshold = unused_threshold\n",
        "        self.printout = printout\n",
        "        self.object_reset_threshold=object_reset_threshold\n",
        "\n",
        "    def track(self,new_boxes):\n",
        "        old_boxes = [obj.get_box() for obj in self.objects]\n",
        "        #old_unmatched = []\n",
        "        remove_oboxes = [False for _ in old_boxes]\n",
        "        used_boxes = [False for _ in new_boxes]\n",
        "        matched = []\n",
        "        for o,obox in enumerate(old_boxes):\n",
        "            #greedy\n",
        "            ious = [self._iou(obox,nbox,ubox) for nbox,ubox in zip(new_boxes,used_boxes)]\n",
        "            #print(ious)\n",
        "            idx = self._max_idx(ious)\n",
        "            \n",
        "            if idx is None:\n",
        "                #old_unmatched.append(self.objects[o])\n",
        "                self.objects[o].unused()\n",
        "                #print(ious,idx,self.objects[o].unused_count)\n",
        "                 # remove lingering boxes\n",
        "                if self.objects[o].get_unused_count() >= self.unused_threshold:\n",
        "                    remove_oboxes[o]=True\n",
        "            else:\n",
        "                used_boxes[idx]=True\n",
        "                self.objects[o].update(new_boxes[idx])\n",
        "\n",
        "        # remove lingering boxes\n",
        "        self.objects = [obj for obj,rbox in zip(self.objects,remove_oboxes) if not rbox]\n",
        "\n",
        "        temp = len(self.objects)\n",
        "        # unmatched new boxes\n",
        "        for nbox,ubox in zip(new_boxes,used_boxes):\n",
        "            if not ubox:\n",
        "                self.objects.append(TrackedObject(self.count,nbox,reset_threshold=self.object_reset_threshold))\n",
        "                self.count+=1\n",
        "        if self.printout:\n",
        "            \n",
        "            info = {\n",
        "                \"OldBoxes\": len(remove_oboxes),\n",
        "                \"RemovedBoxes\": sum(remove_oboxes),\n",
        "                \"Matches\": sum(used_boxes),\n",
        "                \"NewBoxes\": len(new_boxes),\n",
        "                \"NewBoxesAppended\":len(self.objects)-temp,\n",
        "                \"Total\": len(self.objects)\n",
        "            }\n",
        "            for k,v in info.items():\n",
        "                print(f\"{k}:{v}\")\n",
        "            print('='*40)\n",
        "        \n",
        "    def _iou(self,obox,nbox,ubox):\n",
        "        if ubox:\n",
        "            return 0\n",
        "        return iou(*obox,*nbox)\n",
        "\n",
        "    def _max_idx(self,my_list):\n",
        "        # only positive numbers for my_list\n",
        "        best = 1e-5\n",
        "        best_idx = None\n",
        "        for idx,item in enumerate(my_list):\n",
        "            if item > best:\n",
        "                best=item\n",
        "                best_idx = idx\n",
        "        return best_idx\n",
        "    def get_objects(self):\n",
        "        return self.objects\n",
        "\n",
        "def visualize_tracker(im,tracker):\n",
        "    \"\"\"\n",
        "    Visualizes what the tracker is doing\n",
        "    This code is tied to the variables seen in tracker.py \n",
        "    \"\"\"\n",
        "    for obj in tracker.get_objects():\n",
        "        # If a tracked object is not found on current frame, don't display its box\n",
        "        if obj.get_unused_count()>1:\n",
        "            continue\n",
        "\n",
        "        # If an object has crossed the line, color is BLACK\n",
        "        if obj.get_has_crossed():\n",
        "                color = [0,0,0]\n",
        "        # Assign each tracked object a color based on its idx (index number)\n",
        "        else:\n",
        "            i = obj.get_idx() % len(COLORS)\n",
        "            color = COLORS[i]\n",
        "\n",
        "            \n",
        "        box = [int(round(j)) for j in obj.get_box()]\n",
        "        cv2.rectangle(im,box[:2],box[2:],color,3)\n",
        "def visualize_line(im,cross):\n",
        "    \"\"\"\n",
        "    Draws a line on the image\n",
        "    \"\"\"\n",
        "    tl,br = cross\n",
        "    return cv2.line(im,tl,br,[0,255,0],2)\n",
        "def show_points(coords, labels, ax, marker_size=100):\n",
        "    pos_points = coords[labels==1]\n",
        "    neg_points = coords[labels==0]\n",
        "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
        "def show_point(coords, label, ax, marker_size=100):\n",
        "    if label==0:\n",
        "      color='red'\n",
        "    else:\n",
        "      color='green'\n",
        "    ax.scatter([coords[0]], [coords[1]], color=color, marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "def display_text_box(img, text):\n",
        "    \"\"\"\n",
        "    Displays a text box on screen\n",
        "    \"\"\"\n",
        "    # Define some parameters for the text box\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 2\n",
        "    thickness = 2\n",
        "    color = (255, 255, 255) # white color\n",
        "    background_color = (0, 0, 0) # black color\n",
        "    padding = 10 # padding around the text\n",
        "    \n",
        "    # Get the size of the text box and calculate the position\n",
        "    text_size, _ = cv2.getTextSize(text, font, font_scale, thickness)\n",
        "    x = padding\n",
        "    y = img.shape[0] - padding - text_size[1]\n",
        "    \n",
        "    # Draw the text box and the text on top of the image\n",
        "    cv2.rectangle(img, (x, y), (x + text_size[0] + padding, y + text_size[1] + padding), background_color, -1)\n",
        "    cv2.putText(img, text, (x + padding // 2, y + text_size[1] + padding // 2), font, font_scale, color, thickness)\n",
        "def filter_detections(detections,score_min=0.3):\n",
        "    dets = [det for det in detections if det.score>score_min]\n",
        "    return dets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVVPE67e86cz"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Compressed video path\n",
        "compressed_path = \"small_shinjuku_1.mp4\"\n",
        "\n",
        "# Show video\n",
        "mp4 = open(compressed_path,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=600 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBlUlHUV-kZt"
      },
      "outputs": [],
      "source": [
        "imgs = get_still_images_from_video('small_shinjuku_1.mp4',{10,500,600,1300})\n",
        "cv2_imshow(img_grid(imgs))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTrdJE2MMTIL"
      },
      "source": [
        "## Running Images through Detector\n",
        "\n",
        "Let's run these sample images through our object detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4StBvZA-j5A"
      },
      "outputs": [],
      "source": [
        "# LOAD THE MODEL\n",
        "\n",
        "import cv2\n",
        "import mmcv\n",
        "from mmcv.transforms import Compose\n",
        "from mmengine.utils import track_iter_progress\n",
        "from mmdet.registry import VISUALIZERS\n",
        "from mmdet.apis import init_detector, inference_detector\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CONFIG= 'crowdhuman_demo_config.py'\n",
        "CHECKPOINT = 'crowdhuman_epoch_24.pth' # Use this if you did not train your own model\n",
        "\n",
        "# Build the model from a config file and a checkpoint file\n",
        "model = init_detector(CONFIG, CHECKPOINT, device='cuda:0')\n",
        "model.cfg.test_dataloader.dataset.pipeline[0].type = 'LoadImageFromNDArray'\n",
        "test_pipeline = Compose(model.cfg.test_dataloader.dataset.pipeline)\n",
        "\n",
        "# Visualizer is used to show results of detector\n",
        "visualizer = VISUALIZERS.build(model.cfg.visualizer)\n",
        "visualizer.dataset_meta = model.dataset_meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMxcoVdWGBad"
      },
      "outputs": [],
      "source": [
        "def visualize_images(model, images):\n",
        "  vis_images = []\n",
        "  for image in images:\n",
        "    result = inference_detector(model, image, test_pipeline=test_pipeline)\n",
        "\n",
        "    visualizer.add_datasample(\n",
        "            'result',\n",
        "            image,\n",
        "            data_sample=result,\n",
        "            draw_gt=False)\n",
        "    vis_image = visualizer.get_image()\n",
        "    vis_images.append(vis_image)\n",
        "  return vis_images\n",
        "\n",
        "imgs_w_detections = visualize_images(model,imgs)\n",
        "cv2_imshow(img_grid(imgs_w_detections))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl6OKgNSRS15"
      },
      "source": [
        "## Manually Manipulating Detector Results\n",
        "\n",
        "We need access to the coordinates of the boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVHZIbLECLuE"
      },
      "outputs": [],
      "source": [
        "imgs = get_still_images_from_video('small_shinjuku_1.mp4',{10,500,600,1300})\n",
        "result = inference_detector(model, imgs[0], test_pipeline=test_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bquhLB4DQFtm"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYHYp6OrQLak"
      },
      "outputs": [],
      "source": [
        "DEFINITION = {\n",
        "    0:'person'\n",
        "}\n",
        "class Detection:\n",
        "  def __init__(self,box,label,score):\n",
        "    self.box=box\n",
        "    self.label=DEFINITION[label]\n",
        "    self.score=score\n",
        "  def __str__(self):\n",
        "    return(f\"{self.label} | {self.score} | {self.box}\")\n",
        "\n",
        "def get_detections_from_result(result):\n",
        "    instances = result.pred_instances.numpy().cpu()\n",
        "    return [Detection(box,label,score) for box,label,score in zip(instances['bboxes'],instances['labels'],instances['scores'])]\n",
        "\n",
        "detections = get_detections_from_result(result)\n",
        "for detection in detections:\n",
        "  print(detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QixMyW1QRQox"
      },
      "outputs": [],
      "source": [
        "def visualize_boxes(image,dets,color = [0,255,0]):\n",
        "    \"\"\"\n",
        "    Visualizes detections\n",
        "    \"\"\"\n",
        "    new_image = image.copy()\n",
        "    for det in dets:\n",
        "        box = [int(round(j)) for j in det.box]\n",
        "        cv2.rectangle(new_image,box[:2],box[2:],color,3)\n",
        "    return new_image\n",
        "\n",
        "det_img = visualize_boxes(imgs[0],detections)\n",
        "cv2_imshow(det_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX4cwofCSjVd"
      },
      "outputs": [],
      "source": [
        "def filter_detections(detections,score_min=0.3):\n",
        "    dets = [det for det in detections if det.score>score_min]\n",
        "    return dets\n",
        "\n",
        "\n",
        "filtered_detections = filter_detections(detections)\n",
        "det_img = visualize_boxes(imgs[0],filtered_detections)\n",
        "cv2_imshow(det_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6bN1y5sWO92"
      },
      "source": [
        "## Tracking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgsKwbMVQ2Nf"
      },
      "outputs": [],
      "source": [
        "track_imgs = get_still_images_from_video('small_shinjuku_1.mp4',{0,1})\n",
        "cv2_imshow(img_grid(track_imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRmPsX4RXu77"
      },
      "outputs": [],
      "source": [
        "def merge_images(images):\n",
        "    tmp = np.zeros_like(images[0],dtype=np.float32)\n",
        "    for image in images:\n",
        "      tmp += image\n",
        "    return tmp/float(len(images))\n",
        "print(\"This is what the 2 images look like overlayed together\")\n",
        "cv2_imshow(merge_images(track_imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3BEGMg4YDjl"
      },
      "outputs": [],
      "source": [
        "# Detections img0\n",
        "track_results0 = inference_detector(model, track_imgs[0], test_pipeline=test_pipeline)\n",
        "track_detections0 = get_detections_from_result(track_results0)\n",
        "track_detections0 = filter_detections(track_detections0)\n",
        "\n",
        "# Detections img1\n",
        "track_results1 = inference_detector(model, track_imgs[1], test_pipeline=test_pipeline)\n",
        "track_detections1 = get_detections_from_result(track_results1)\n",
        "track_detections1 = filter_detections(track_detections1)\n",
        "\n",
        "\n",
        "tmp = track_imgs[1].copy()\n",
        "tmp = visualize_boxes(tmp,track_detections0,color=[0,255,0])\n",
        "tmp = visualize_boxes(tmp,track_detections1,color=[255,0,0])\n",
        "print(\"Frame0 Detections in Green. Frame1 Detections in Blue\")\n",
        "cv2_imshow(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQTyCmEOZ2h-"
      },
      "outputs": [],
      "source": [
        "# Choose a random person from frame0 and calculate IOU for each box in frame1\n",
        "import random\n",
        "import numpy as np\n",
        "def iou(x1_1, y1_1, x2_1, y2_1,\n",
        "        x1_2, y1_2, x2_2, y2_2):\n",
        "    intersection_width = max(min(x2_1, x2_2) - max(x1_1, x1_2),0)\n",
        "    intersection_height = max(min(y2_1, y2_2) - max(y1_1, y1_2),0)\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "    return intersection_area / union_area\n",
        "\n",
        "my_person = track_detections0[random.randint(0,len(track_detections0)-1)]\n",
        "ious = [iou(*my_person.box,*det.box) for det in track_detections1]\n",
        "print(\"IOUs\")\n",
        "print(ious)\n",
        "\n",
        "print(\"\")\n",
        "best_iou_index = np.argmax(np.array(ious))\n",
        "print(f\"Best IOU: {ious[best_iou_index]} at index {best_iou_index}\")\n",
        "\n",
        "my_matched_person = track_detections1[best_iou_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MUYAaH2sVeN"
      },
      "outputs": [],
      "source": [
        "# To find another random person, re-run previous cell and this one\n",
        "tmp = np.zeros_like(track_imgs[1])\n",
        "tmp = visualize_boxes(tmp,track_detections1,color=[255,0,0])\n",
        "tmp = visualize_boxes(tmp,[my_person],color=[0,255,0])\n",
        "tmp = visualize_boxes(tmp,[my_matched_person],color=[0,100,0])\n",
        "print(\"All frame1 boxes in blue.\\nRandom person from frame0 in light green.\\nThe frame1 matched box in dark green\")\n",
        "cv2_imshow(tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkvUyxXuxBEG"
      },
      "outputs": [],
      "source": [
        "my_tracked_person = None\n",
        "track_imgs = get_still_images_from_video('small_shinjuku_1.mp4',{0,1})\n",
        "fig, AX = plt.subplots()\n",
        "AX.cla()\n",
        "AX.set_title(\"Select a person\")\n",
        "tmp=track_imgs[0].copy()\n",
        "tmp=visualize_boxes(tmp,track_detections0,color=[0,255,0])\n",
        "tmp = tmp[:,:,::-1]\n",
        "AX.imshow(tmp)\n",
        "def find_closest_box(dets,coords):\n",
        "  closest = 10e4\n",
        "  closets_index = -1\n",
        "  ix,iy = coords\n",
        "  for i,det in enumerate(dets):\n",
        "    x0,y0,x1,y1 = det.box\n",
        "    x,y = (x0+x1)/2, (y0+y1)/2\n",
        "    dist = np.sqrt(((x-ix)**2) + ((y-iy)**2))\n",
        "    if dist < closest:\n",
        "      closest = dist\n",
        "      closest_index = i\n",
        "  return closest_index\n",
        "\n",
        "def show_point(coords, label, ax, marker_size=100):\n",
        "    if label==0:\n",
        "      color='red'\n",
        "    else:\n",
        "      color='green'\n",
        "    ax.scatter([coords[0]], [coords[1]], color=color, marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
        "def onclick(event):\n",
        "    global AX, track_imgs,track_detections0,my_tracked_person\n",
        "    ix, iy = event.xdata, event.ydata\n",
        "    if ix and iy:\n",
        "      coords = [ix,iy]\n",
        "      \n",
        "      i=find_closest_box(track_detections0,coords)\n",
        "      my_tracked_person = track_detections0[i]\n",
        "      tmp=track_imgs[0].copy()\n",
        "      tmp=visualize_boxes(tmp,track_detections0,color=[0,255,0])\n",
        "      tmp=visualize_boxes(tmp,[my_tracked_person],color=[0,0,255])\n",
        "      tmp=tmp[:,:,::-1]\n",
        "      AX.cla()\n",
        "      AX.set_title(\"Select a person\")\n",
        "      AX.imshow(tmp)\n",
        "      show_point(coords,1,AX)      \n",
        "\n",
        "cid = fig.canvas.mpl_connect('button_press_event', onclick)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzav9vH651yJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"  \n",
        "Basic Code for Opening and Reading a Video/Camera in OpenCV\n",
        "This is just a code sample\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cam = cv2.VideoCapture(NAME_OF_VIDEO)\n",
        "while True:\n",
        "  ret,img = cam.read()\n",
        "  # If reaches end of video, or no image available, exist while loop\n",
        "  if not ret:\n",
        "    break\n",
        "  \n",
        "  cv2.imshow('window',img)\n",
        "  chd=cv2.waitKey(1)\n",
        "  # Break if q is pressed\n",
        "  if chd==ord('q'):\n",
        "    break\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L77f0zZR2aeZ"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "from copy import deepcopy\n",
        "def run_video_file(vid_name,tracked_person,max_frames=150,output='tmp'):\n",
        "    cam = cv2.VideoCapture(vid_name)\n",
        "    vw = VideoWriter(output)\n",
        "\n",
        "    frame_num = 0\n",
        "    while frame_num < max_frames:\n",
        "      ret,img = cam.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      # Run object detector on image\n",
        "      result = inference_detector(model, img, test_pipeline=test_pipeline)\n",
        "      detections = get_detections_from_result(result)\n",
        "      detections = filter_detections(detections)\n",
        "\n",
        "      if tracked_person is not None:\n",
        "        # Calculate IOU for each new detection\n",
        "        ious = [iou(*tracked_person.box,*det.box) for det in detections]\n",
        "        best_iou_index = np.argmax(np.array(ious))\n",
        "        best_iou = ious[best_iou_index]\n",
        "        # tracked person is updated based on iou\n",
        "        tracked_person = detections[best_iou_index]\n",
        "\n",
        "        #If iou is not a positive value, there is no match\n",
        "        if best_iou <1e-5:\n",
        "          tracked_person = None\n",
        "      \n",
        "      if tracked_person is not None:\n",
        "        img = visualize_boxes(img,[tracked_person],[0,0,255])\n",
        "      vw.write(img)\n",
        "\n",
        "      frame_num+=1\n",
        "    vw.release()\n",
        "\n",
        "FILENAME = 'small_shinjuku_1.mp4'\n",
        "OUTPUT = 'tmp'\n",
        "run_video_file(FILENAME,deepcopy(my_tracked_person),output=OUTPUT)\n",
        "      \n",
        "# Show video\n",
        "mp4 = open(OUTPUT+'.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=600 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvC_8mzgsXjx"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def iou(x1_1, y1_1, x2_1, y2_1,\n",
        "        x1_2, y1_2, x2_2, y2_2):\n",
        "    intersection_width = max(min(x2_1, x2_2) - max(x1_1, x1_2),0)\n",
        "    intersection_height = max(min(y2_1, y2_2) - max(y1_1, y1_2),0)\n",
        "    intersection_area = intersection_width * intersection_height\n",
        "    box1_area = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
        "    box2_area = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "    return intersection_area / union_area\n",
        "class TrackedObject:\n",
        "    def __init__(self,idx,box,reset_threshold=60):\n",
        "        self.idx = idx\n",
        "        self.box = box\n",
        "        self.unused_count = 0\n",
        "        self.start_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "        self.current_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]\n",
        "        self.has_crossed = False\n",
        "        self.direction = ''\n",
        "        self.reset_threshold = reset_threshold\n",
        "        self.cross_count = 0\n",
        "\n",
        "    def update(self,box):\n",
        "        self.box = box\n",
        "        self.unused_count = 0\n",
        "        self.current_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "        if self.has_crossed:\n",
        "            self.cross_count+=1\n",
        "            if self.cross_count>=self.reset_threshold:\n",
        "                self.has_crossed=False\n",
        "                self.start_centroid = [(box[0]+box[2])/2,(box[1]+box[3])/2]#x,y\n",
        "    def unused(self):\n",
        "        self.unused_count+=1\n",
        "    def get_has_crossed(self):\n",
        "        return self.has_crossed\n",
        "    def mark_crossed(self):\n",
        "        self.has_crossed = True\n",
        "        self.cross_count = 0\n",
        "        self.up_or_down()\n",
        "    def up_or_down(self):\n",
        "        y0 = self.start_centroid[1]\n",
        "        y1 = self.current_centroid[1]\n",
        "        self.direction='up'\n",
        "        if y1>y0:\n",
        "            self.direction = 'down'\n",
        "    def get_unused_count(self):\n",
        "        return self.unused_count\n",
        "    def get_direction(self):\n",
        "        return self.direction\n",
        "    def get_box(self):\n",
        "        return self.box\n",
        "    def get_idx(self):\n",
        "        return self.idx\n",
        "    def get_has_crossed(self):\n",
        "        return self.has_crossed\n",
        "    def get_start_centroid(self):\n",
        "        return self.start_centroid\n",
        "    def get_current_centroid(self):\n",
        "        return self.current_centroid\n",
        "    def get_cross_count(self):\n",
        "        return self.cross_count\n",
        "class Tracker:\n",
        "    def __init__(self,unused_threshold=10,printout=False,object_reset_threshold=100):\n",
        "        self.objects = []\n",
        "        self.count = 0\n",
        "        self.unused_threshold = unused_threshold\n",
        "        self.printout = printout\n",
        "        self.object_reset_threshold=object_reset_threshold\n",
        "\n",
        "    def track(self,new_boxes):\n",
        "        old_boxes = [obj.get_box() for obj in self.objects]\n",
        "        #old_unmatched = []\n",
        "        remove_oboxes = [False for _ in old_boxes]\n",
        "        used_boxes = [False for _ in new_boxes]\n",
        "        matched = []\n",
        "        for o,obox in enumerate(old_boxes):\n",
        "            #greedy\n",
        "            ious = [self._iou(obox,nbox,ubox) for nbox,ubox in zip(new_boxes,used_boxes)]\n",
        "            #print(ious)\n",
        "            idx = self._max_idx(ious)\n",
        "            \n",
        "            if idx is None:\n",
        "                #old_unmatched.append(self.objects[o])\n",
        "                self.objects[o].unused()\n",
        "                #print(ious,idx,self.objects[o].unused_count)\n",
        "                 # remove lingering boxes\n",
        "                if self.objects[o].get_unused_count() >= self.unused_threshold:\n",
        "                    remove_oboxes[o]=True\n",
        "            else:\n",
        "                used_boxes[idx]=True\n",
        "                self.objects[o].update(new_boxes[idx])\n",
        "\n",
        "        # remove lingering boxes\n",
        "        self.objects = [obj for obj,rbox in zip(self.objects,remove_oboxes) if not rbox]\n",
        "\n",
        "        temp = len(self.objects)\n",
        "        # unmatched new boxes\n",
        "        for nbox,ubox in zip(new_boxes,used_boxes):\n",
        "            if not ubox:\n",
        "                self.objects.append(TrackedObject(self.count,nbox,reset_threshold=self.object_reset_threshold))\n",
        "                self.count+=1\n",
        "        if self.printout:\n",
        "            \n",
        "            info = {\n",
        "                \"OldBoxes\": len(remove_oboxes),\n",
        "                \"RemovedBoxes\": sum(remove_oboxes),\n",
        "                \"Matches\": sum(used_boxes),\n",
        "                \"NewBoxes\": len(new_boxes),\n",
        "                \"NewBoxesAppended\":len(self.objects)-temp,\n",
        "                \"Total\": len(self.objects)\n",
        "            }\n",
        "            for k,v in info.items():\n",
        "                print(f\"{k}:{v}\")\n",
        "            print('='*40)\n",
        "        \n",
        "    def _iou(self,obox,nbox,ubox):\n",
        "        if ubox:\n",
        "            return 0\n",
        "        return iou(*obox,*nbox)\n",
        "\n",
        "    def _max_idx(self,my_list):\n",
        "        # only positive numbers for my_list\n",
        "        best = 1e-5\n",
        "        best_idx = None\n",
        "        for idx,item in enumerate(my_list):\n",
        "            if item > best:\n",
        "                best=item\n",
        "                best_idx = idx\n",
        "        return best_idx\n",
        "    def get_objects(self):\n",
        "        return self.objects\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XjK0NS1P4E_a"
      },
      "outputs": [],
      "source": [
        "def run_tracking(vid_name,max_frames=150,output='tmp'):\n",
        "    cam = cv2.VideoCapture(vid_name)\n",
        "    vw = VideoWriter(output)\n",
        "\n",
        "    tracker = Tracker()\n",
        "\n",
        "    frame_num = 0\n",
        "    while frame_num < max_frames:\n",
        "      ret,img = cam.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      # Run object detector on image\n",
        "      result = inference_detector(model, img, test_pipeline=test_pipeline)\n",
        "      detections = get_detections_from_result(result)\n",
        "      detections = filter_detections(detections)\n",
        "\n",
        "      #Track the object boxes\n",
        "      tracker.track([det.box for det in detections])\n",
        "      # visualize tracker\n",
        "      visualize_tracker(img,tracker)\n",
        "      vw.write(img)\n",
        "\n",
        "      frame_num+=1\n",
        "    vw.release()\n",
        "\n",
        "FILENAME = 'small_shinjuku_1.mp4'\n",
        "OUTPUT = 'tmp'\n",
        "run_tracking(FILENAME,output=OUTPUT)\n",
        "      \n",
        "# Show video\n",
        "mp4 = open(OUTPUT+'.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=600 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counting"
      ],
      "metadata": {
        "id": "sKCv80bCIXAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X57OXx4jHn4a"
      },
      "outputs": [],
      "source": [
        "CLICKED_POINTS = []\n",
        "BOUNDARIES = []\n",
        "first_image = get_still_images_from_video('small_shinjuku_1.mp4',{0})[0]\n",
        "fig, AX1 = plt.subplots()\n",
        "AX1.cla()\n",
        "AX1.set_title(\"Select 2 points to create boundary\")\n",
        "tmp=first_image.copy()\n",
        "tmp=tmp[:,:,::-1]\n",
        "AX1.imshow(tmp)\n",
        "\n",
        "def onclick_boundary(event):\n",
        "    global AX1, CLICKED_POINTS,first_image,BOUNDARIES\n",
        "    ix, iy = event.xdata, event.ydata\n",
        "    AX1.cla()\n",
        "    AX1.set_title(\"Select 2 points to create boundary\")\n",
        "    if ix and iy:\n",
        "      coords = [int(ix),int(iy)]\n",
        "      CLICKED_POINTS.append(coords)\n",
        "      if len(CLICKED_POINTS)>=2:\n",
        "        BOUNDARIES.append(CLICKED_POINTS[:2])\n",
        "        CLICKED_POINTS = []\n",
        "      elif len(CLICKED_POINTS)==1:\n",
        "        show_point(CLICKED_POINTS[0],1,AX1)\n",
        "    tmp=first_image.copy()\n",
        "    for tmp_points in BOUNDARIES:\n",
        "      visualize_line(tmp,tmp_points)\n",
        "    tmp=tmp[:,:,::-1]\n",
        "    AX1.imshow(tmp)\n",
        "\n",
        "cid = fig.canvas.mpl_connect('button_press_event', onclick_boundary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"You have {len(BOUNDARIES)} boundaries with coordinates below:\\n\")\n",
        "for boundary in BOUNDARIES:\n",
        "  print(boundary)"
      ],
      "metadata": {
        "id": "7RptFqwBaSU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Counter:\n",
        "    def __init__(self,boundary,direction_type='up-down'):\n",
        "        assert direction_type in {'up-down','left-right'}\n",
        "        self.direction_type=direction_type\n",
        "        self.boundary = boundary # point0, point2\n",
        "        self.reset()\n",
        "    def check_crosses(self,objs):\n",
        "        for obj in objs:\n",
        "            if not obj.get_has_crossed():\n",
        "                cross_direction = self.check_cross(obj)\n",
        "                if cross_direction != 'None':\n",
        "                    obj.mark_crossed()\n",
        "                    self.totals[cross_direction]+=1\n",
        "    def reset(self):\n",
        "        self.totals = {'left':0,'right':0}\n",
        "        if self.direction_type=='up-down':\n",
        "          self.totals = {'up':0,'down':0}\n",
        "        \n",
        "    def check_cross(self,obj):\n",
        "        x0_0, y0_0 = self.boundary[0]\n",
        "        x1_0, y1_0 = self.boundary[1]\n",
        "        x0_1, y0_1 = obj.get_start_centroid()\n",
        "        x1_1, y1_1 = obj.get_current_centroid()\n",
        "        dx0 = x1_0 - x0_0\n",
        "        dy0 = y1_0 - y0_0\n",
        "        dx1 = x1_1 - x0_1\n",
        "        dy1 = y1_1 - y0_1\n",
        "        denominator = dx1 * dy0 - dy1 * dx0\n",
        "        if denominator == 0:\n",
        "            return 'None'  # lines are parallel\n",
        "        t = ((x0_0 - x0_1) * dy1 - (y0_0 - y0_1) * dx1) / denominator\n",
        "        u = ((x0_0 - x0_1) * dy0 - (y0_0 - y0_1) * dx0) / denominator\n",
        "        if 0 <= t <= 1 and 0 <= u <= 1:\n",
        "            # UP DOWN\n",
        "            if self.direction_type=='up-down':\n",
        "                if y1_1 > y0_1:\n",
        "                    return 'down'\n",
        "                return 'up'\n",
        "            # LEFT RIGHT\n",
        "            if x1_1 > x0_1:\n",
        "                return 'right'\n",
        "            return 'left'            \n",
        "        return 'None'  # segments do not intersect\n",
        "    def get_results(self):\n",
        "        return self.totals\n",
        "    def get_boundary(self):\n",
        "        return self.boundary\n",
        "    "
      ],
      "metadata": {
        "id": "IhdgLR5ecZgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_counting(vid_name,boundaries,max_frames=300,output='tmp'):\n",
        "    cam = cv2.VideoCapture(vid_name)\n",
        "    vw = VideoWriter(output)\n",
        "\n",
        "    tracker = Tracker()\n",
        "\n",
        "    counters = [Counter(boundary) for boundary in boundaries]\n",
        "\n",
        "    frame_num = 0\n",
        "    while frame_num<max_frames:\n",
        "      ret,img = cam.read()\n",
        "      if not ret:\n",
        "        break\n",
        "      # Run object detector on image\n",
        "      result = inference_detector(model, img, test_pipeline=test_pipeline)\n",
        "      detections = get_detections_from_result(result)\n",
        "      detections = filter_detections(detections)\n",
        "\n",
        "      #Track the object boxes\n",
        "      tracker.track([det.box for det in detections])\n",
        "      visualize_tracker(img,tracker)\n",
        "\n",
        "      #Count\n",
        "      for counter in counters:\n",
        "        visualize_line(img,counter.get_boundary())\n",
        "        counter.check_crosses(tracker.get_objects())\n",
        "      counts = [counter.get_results() for counter in counters]\n",
        "\n",
        "      # Get counts and display that on image\n",
        "      text=' | '.join([f\"{count}\" for count in counts])\n",
        "      display_text_box(img,text)\n",
        "\n",
        "      vw.write(img)\n",
        "\n",
        "      frame_num+=1\n",
        "    vw.release()\n",
        "\n",
        "\n",
        "FILENAME = 'small_shinjuku_1.mp4'\n",
        "OUTPUT = 'tmp'\n",
        "run_counting(FILENAME,BOUNDARIES,output=OUTPUT)\n",
        "      \n",
        "# Show video\n",
        "mp4 = open(OUTPUT+'.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=600 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "9lMGfyInil6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dU9XT2vfstfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}